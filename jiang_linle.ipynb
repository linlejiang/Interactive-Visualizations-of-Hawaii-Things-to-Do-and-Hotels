{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INF 510 Fall 2019 Final Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\t**The names of team member(s)**:\n",
    "\n",
    "    Linle Jiang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\t**How to run your code (what command-line switches they are, what happens when you invoke the code, etc.)**\n",
    "\n",
    "    This project requires the following packages:\n",
    "    - requests, BeautifulSoup, pandas, json, csv, argparse and bokeh\n",
    "    \n",
    "    To use the Bokeh package, please make sure you activate the environment file.\n",
    "    \n",
    "    To run this project, make sure the above packages are installed, and then simply clone the repo at https://github.com/linlejiang/inf550_project and execute the python file using command line. The -source=remote will scrape the place data from Google and TripAdvisor, and then access Google Place API to obtain the geolocation of the places. On the other hand, the -source=local will just used the processed data generated from the -source=remote code. With the data prepared, both will then proceed to the analysis section. \n",
    "    \n",
    "    Note: please copy the Google API_key from the DEN Dropbox submission and insert it into the API_key variable below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\t**Any major “gotchas” to the code (i.e. things that don’t work, go slowly, could be improved, etc.)**\n",
    "    \n",
    "    The code takes about 10 minutes to scrap the data from Google and TripAdvisor.  And the generated interative plots will all in the same row, I couldn't make them display in two rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  **Anything else you feel is relevant to the grading of your project your project.**\n",
    "\n",
    "    If you don't run the remote mode before the local mode, you may find that the two mode generates slightly different hotel data. Specifically, the number of hotels may differ. For more details and explanations, please refer to the comments under the 'Hotel data criteria' in the trip_data() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **What did you set out to study?  (i.e. what was the point of your project?  This should be close to your Milestone 1 assignment, but if you switched gears or changed things, note it here.)**\n",
    "\n",
    "    Consistent with my Milestone 1 assignment, my motivation to select this project is to plan a trip to Hawaii. From my experience, there isn’t any interactive maps with both top sights and hotels plotted simultaneously or without substantial manual efforts. Therefore, the ultimate goals of this project are: 1) to create three interactive maps of Hawaii automatically, which will enable users to interact with the maps (e.g., the size of maps can be changed within the visualization, and when the mouse cursor is at a hotel datapoint, the name of the hotel will show up); and 2) to compare visually how different the distributions would be for hotels plotted by different popularity metrics.\n",
    "\n",
    "    However, there are some minor changes: \n",
    "\n",
    "      1. the hotel address data was planned to be scraped from TripAdvisor initially, in order to use Google API to get geometry results. However, this plan was dropped due to the following reasons:     \n",
    "          1) some hotel addresses obtained from TripAdvisor were not correct, after check the website;  \n",
    "          2) Google API allows for hotel names as inputs to request hotel geometry, and using this approach substantially decrease the number of missing data than scraping from TripAdvisor;          \n",
    "          3) this increases the data acquiring time significantly, and lower the probability of being banned by the website, as the number of requests drop substantially.\n",
    "\n",
    "      2. I plot 20 sights instead of 10.\n",
    "\n",
    "      3. Instead of collecting all hotel listings from TripAdvisor, I specified certain criteria to determine what kinds of hotels should be included in this dataset. This is to resemble real-life hotel selection processes, and to limit the number of request queries when the code scrapes data from TripAdvisor.\n",
    "\n",
    "      4. The three metrics used to describe the hotels are price, popularity (i.e., number of reviews), and recommendation (i.e., computed by the product of the ratings and the standardized number of reviews). The recommendation metric is also used to select the sights in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **What did you Discover/what were your conclusions (i.e. what were your findings?  Were your original assumptions confirmed, etc.?)**\n",
    "\n",
    "    I generate three interative plots in a html file, if you run the code, they will automatically show up in your browser. The plot enables users to interact with the maps (1. the size of maps can be changed; 2. when the mouse cursor is at a sight, the name of the sight will show up; 3. when the mouse cursor is at a hotel, the name and lowest price of the hotel will show up; 4. when click on the hotel circle, the browser will open a new tab directing the user to the hotel webpage in TripAdvisor; 5. the map can zoom in/out if you launch the wheelZoom tool; 6. you can drag the map to view different areas within the map).\n",
    "    \n",
    "    Overall, in this project, I found that the most recommended sights (i.e., the metric based on reviews and ratings) are concentrated in the urban area, only a few of them are natural sights. Also, although all the islands have at least one most recommended sight, most of them are in Honolulu. This makes sense, considering that Honolulu is the most developed area in Hawaii islands. \n",
    "    \n",
    "    For the hotels included in the dataset, first, most of them are located near the sea. This is consistent with the idea that the general public enjoys the beach and hotels with a view of the sea. Additionally, it appears that the cheaper hotels tend to cluster together, likely due to the high demand and competitive nature, while the more expensive hotels are more spread out. Also, only about half of the sights have hotels closeby, this is especially the case in the Island of Hawaii and the urban area of Honolulu. Moreover, these hotels closer to the sights tend to have lower prices. Finally, it appears that there are positive correlations between hotel price, popularity (i.e., number of reviews), and recommendation. Consider the higher expenses in Hawaii, it is possible that visitors of Hawaii tend to have higher income level. And thus they also tend to stay at hotels with higher quality, and are able to afford such expenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **What difficulties did you have in completing the project?**  \n",
    "\n",
    "    *What didn't work?  What was hard to do?  What stumbling blocks did you run into?*\n",
    "    \n",
    "    The most difficult part for me is to scrape a large amount of data from TripAdvisor. According to my initial plan, I would have to make thousands of request to the website in order to get all the hotel data. To avoid being banned by the website, I tried to rotate proxies and user agents. However, that was not practical because the free public IP significantly slowed down the scraping processes, which is likely due to the extended time making requests via different proxies. Therefore, I had to step back and figure out what were the data that might not be necessary. Fortunately, I was able to increase the number of hotels to be included in this dataset by removing one unecessary attribute (i.e., hotel address, note that each address needs one individual request when scraping).\n",
    "    \n",
    "    Another difficulty I had was to optimize my code to shorten the time spent on scraping the hotel data. However, my hand is tied given the website structure, which requires a large amount of requests to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **What skills did you wish you had while you were doing the project?**\n",
    "\n",
    "    *Was there something that you wish you'd have known better while you were doing the project?  If you learned that skill while doing the project, note it here, but even if not, what would have helped?*\n",
    "\n",
    "    As I mentioned in question 7, first, I wished I had learned how to optimize my code to collect data at a faster rate, but I wasn't sure if there are ways to make my code more efficient. Second, I wished I had learned how to perform proxy rotation and user agent rotation to prevent being banned by the website. I did learn some techniques and applied, but they weren't successful attempts. Third, I wished I had known how to create interative visualizations, which, through this project, I learned about how to use the bokeh package to create interactive maps. I was able to include all features as planned. Still, there are a lot more useful and interesting features to be learned. Finally, I wished I had learned some NLP techniques, so that I could collect the reviews for each hotel, and extract keywords from the review to provide some qualitative descriptions for the each hotel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **What would you do “next” to expand or augment the project?**\n",
    "\n",
    "    *If you had to continue this project, what would you add to it?  If you had the skills you mentioned in question 8, what could you do to enhance things?*\n",
    "    \n",
    "    First, I would extend the program so that whenever people searches for a location, the program will automatically scrape data for that location and generate the interactive maps. Second, if I had known how to optimize my code and perform efficient proxies and user agent rotations, my program would scale better and be able to collect more data and operate faster. Third, if I had learned some NLP keyword extraction technique, I could include some extra attributes in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da01e031ae8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayouts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mAPI_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Google API Key: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import ColumnDataSource, GMapOptions, LinearColorMapper, ColorBar, HoverTool, TapTool, OpenURL\n",
    "from bokeh.plotting import gmap\n",
    "from bokeh.transform import transform\n",
    "from bokeh.layouts import row\n",
    "\n",
    "API_key = input('Google API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soup data from website\n",
    "def get_trip_soup(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"There is a problem connecting you to the website.\")\n",
    "    else:\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google sight data scraper\n",
    "def google_data(soup):\n",
    "    div_all_results = soup.findAll('div', {'class':'GwjAi'})\n",
    "    sight_data = {}\n",
    "    \n",
    "    for sights in div_all_results:\n",
    "        sight_name = sights.find('div', {'class':'skFvHc'}).contents[0]\n",
    "        sight_data[sight_name] = []\n",
    "        \n",
    "        # some of the sights may not have ratings or reviews, so need to check if that data is available before adding it\n",
    "        if sights.find('span', {'class':'KFi5wf'}) != None:\n",
    "            sight_rating = float(sights.find('span', {'class':'KFi5wf'}).contents[0])  # ratings are float, convert from str\n",
    "        else:\n",
    "            sight_rating = 0.0\n",
    "        \n",
    "        review = []\n",
    "        if sights.find('span', {'class':'jdzyld'}) != None:\n",
    "            sight_review = sights.find('span', {'class':'jdzyld'}).contents[0]\n",
    "        else:\n",
    "            sight_review = '0'\n",
    "        \n",
    "        # sight_review is a Navigatestring object, this is to format it\n",
    "        # using .lstrip() can convert it back to regular string\n",
    "        review = list(sight_review.lstrip())\n",
    "        for i in review:\n",
    "            for j in ['(', ')', ',']:\n",
    "                if j in review:\n",
    "                    review.remove(j)\n",
    "        review_str = ''.join(review)\n",
    "        \n",
    "        sight_data[sight_name] = [sight_rating, int(review_str)]  # review_str are int, convert from str\n",
    "\n",
    "    return sight_data  # return a dict, each key represents a sight, and the value holds data for that sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TripAdvisor hotel data scraper\n",
    "def trip_data(trip_soup):\n",
    "\n",
    "    \"\"\"logic:\n",
    "\n",
    "    1) The hotel data are organized by regions (i.e., cities), so the first step of this function is to get all the \n",
    "    links for the regions. The regions are scattered in three pages.\n",
    "\n",
    "    Note that the format of the first page and the rest are different.\n",
    "\n",
    "    2) Each region may have multiple pages of hotel data, so the second step is to, after request a region page, \n",
    "    collect hotel data from a page, and then iterate through all the pages. Iterate requesting the regions pages.\n",
    "\n",
    "    Note that the hotel address data was planned to be scraped from TripAdvisor initially, in order to use Google\n",
    "    API to get geometry results. However, this plan was dropped in this project due to the following reasons:\n",
    "        1) some hotel addresses obtained from TripAdvisor were not correct, after check the website;\n",
    "        2) Google API allows for hotel names as inputs to request hotel geometry, and using this approach substantially\n",
    "        decrease the number of missing data than scraping from TripAdvisor;\n",
    "        3) this increases the data acquiring time significantly, and lower the probability of being banned by the\n",
    "        website, as the number of requests drop substantially.\n",
    "\n",
    "    Hotel data criteria:\n",
    "\n",
    "    Due to the huge amount of data, data is included only if it is with: \n",
    "        1) more than 100 reviews are included;\n",
    "        2) available and qualified user rating (>= 3.0);\n",
    "        3) available price listing.\n",
    "    And the hotel listing is not sponsored (i.e., ad). With this criterion, it may lead to inconsistent number of\n",
    "    hotel data. Because sponsored hotel will not be listed again in regular hotel listings, and there are different\n",
    "    sponsored hotels, with different hotel criteria as listed aforementioned.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1st. get all regions (i.e., city) links for later access\n",
    "    # since the regions links differ between the ones from the first page and from the rest pages, get them separately\n",
    "\n",
    "    # for regions links in page 1\n",
    "    regions_div = trip_soup.findAll('div', {'class': 'geo_wrap'})\n",
    "    regions_list = []\n",
    "\n",
    "    # get the region links\n",
    "    for regions in regions_div:\n",
    "        if regions.find('a', {'class': 'linkText'}) != None:\n",
    "            region_sublink = regions.find('a').get('href')\n",
    "            region_links = 'https://www.tripadvisor.com' + region_sublink\n",
    "            regions_list.append(region_links)\n",
    "\n",
    "    # for regions links in the following pages\n",
    "    next_region_page_soup = trip_soup\n",
    "    while True:\n",
    "        region_page_sublink = next_region_page_soup.find('a', {'class': 'nav next taLnk ui_button primary'})\n",
    "\n",
    "        if region_page_sublink == None:  # stop if no more next page available\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            next_regions_link = 'https://www.tripadvisor.com' + region_page_sublink.get('href')\n",
    "            requests.packages.urllib3.disable_warnings()  # this is to prevent error causing by SSL verification\n",
    "            next_region_page_soup = get_trip_soup(next_regions_link)\n",
    "\n",
    "            next_regions_li = next_region_page_soup.findAll('li', {'class': 'ui_column is-12-mobile is-4-tablet is-3-desktop'})\n",
    "\n",
    "            for next_regions in next_regions_li:\n",
    "                if next_regions.find('a', {'class': 'city'}) != None:\n",
    "                    next_regions_sublink = next_regions.find('a').get('href')\n",
    "                    next_regions_links = 'https://www.tripadvisor.com' + next_regions_sublink\n",
    "                    regions_list.append(next_regions_links)\n",
    "\n",
    "    # 2nd. get all hotel pages link from each region link (each region link may contain multiple pages of hotel data)\n",
    "    # when each page link is obtained, create new page soup to get hotel data at the same time\n",
    "\n",
    "    hotel_data = {}  # hotel names are keys, all other data (including hotel_url, hotel_price, hotel_rating, \n",
    "                    # hotel_review) are stored in a list as values\n",
    "\n",
    "    for regions in regions_list:\n",
    "        page_soup = get_trip_soup(regions)\n",
    "\n",
    "        while True:\n",
    "            # hotel data on every following page\n",
    "            div_all_results = page_soup.findAll('div', {'class':'ui_column is-8 main_col allowEllipsis'})\n",
    "\n",
    "            for hotels in div_all_results:\n",
    "\n",
    "                # get hotel name:\n",
    "                try:\n",
    "                    \n",
    "                    if hotels.find('a', {'dir': 'ltr'}).contents[0] != None:\n",
    "                        hotel_name = hotels.find('a', {'dir': 'ltr'}).contents[0]\n",
    "                    \n",
    "                except AttributeError:\n",
    "                    break\n",
    "                        \n",
    "                except TypeError:\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    # get number of reviews, definately has a value for each hotel (equal or greater than 0)\n",
    "                    # format the reviews from NavigatableString to int\n",
    "                    review = list(hotels.find('a', {'class': 'review_count'}).contents[0].split(' ')[0])\n",
    "                    if ',' in review:\n",
    "                        review.remove(',')\n",
    "                    hotel_review = int(''.join(review))\n",
    "\n",
    "                    # hotel criteria with number of reviews\n",
    "                    if hotel_name not in hotel_data and hotel_review > 100:\n",
    "                            \n",
    "                        try:\n",
    "\n",
    "                            # hotel criteria with ratings, if no ratings, the attribute will not occur, so need to catch that exception\n",
    "                            if hotels.find('a', {'data-clicksource': 'BubbleRating'}).attrs['alt'].split(' ')[0] != None and float(hotels.find('a', {'data-clicksource': 'BubbleRating'}).attrs['alt'].split(' ')[0]) >= 3.0:\n",
    "                                hotel_rating = float(hotels.find('a', {'data-clicksource': 'BubbleRating'}).attrs['alt'].split(' ')[0])\n",
    "                                \n",
    "                        except AttributeError:\n",
    "                                break\n",
    "                                \n",
    "                        else:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                # hotel criteria with price listings, if not available, it will indicate TypeError, so need to catch that exception\n",
    "                                if hotels.find('div', {'class': 'price autoResize'}) != None:\n",
    "                                    price = hotels.find('div', {'class': 'price autoResize'}).contents[0]\n",
    "                                    list_price = list(price.lstrip())\n",
    "                                        \n",
    "                            except TypeError:\n",
    "                                break\n",
    "                                        \n",
    "                            else:        \n",
    "                                \n",
    "                                # formating price to float from NavigatableString\n",
    "                                for i in ['$', ',']:\n",
    "                                    if i in list_price:\n",
    "                                        list_price.remove(i)\n",
    "                                hotel_price = int(''.join(list_price))\n",
    "\n",
    "                                # get hotel url\n",
    "                                url = hotels.find('a').get('href')\n",
    "                                hotel_url = 'https://www.tripadvisor.com' + url\n",
    "    \n",
    "                                # relevant hotel data are stored as lists\n",
    "                                hotel_d = [hotel_url, hotel_price, hotel_rating, hotel_review]\n",
    "                                hotel_data[hotel_name] = hotel_d\n",
    "                    \n",
    "            # find the next page in the current region\n",
    "            page_sublink = page_soup.find('a', {'class':'nav next taLnk ui_button primary'})   \n",
    "\n",
    "            if page_sublink == None:  # if no more next page available, break\n",
    "                break\n",
    "\n",
    "            else:  # get the link for next page in the current region\n",
    "                page_link = 'https://www.tripadvisor.com' + page_sublink.get('href')\n",
    "                page_soup = get_trip_soup(page_link)\n",
    "                \n",
    "    return hotel_data  # returns a dict of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Google API to get geometry data for sights and hotels, the results are in json format\n",
    "def get_geometry(place):\n",
    "    end_point = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json'\n",
    "    params = {\n",
    "        'input': place,\n",
    "        'inputtype': 'textquery',\n",
    "        'key': API_key,\n",
    "        'fields': 'geometry'\n",
    "    }\n",
    "    req = requests.get(end_point, params = params)\n",
    "    results = req.json()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access Google Place API to obtain the geometry data of the sights and write all the attributes to a csv file\n",
    "def request_sight_data():\n",
    "    google_url = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502405%2C2502548%2C4208993%2C4254308%2C4258168%2C4260007%2C4270442%2C4274032%2C4285990%2C4288513%2C4289525%2C4291318%2C4296668%2C4301054%2C4303479%2C4305595%2C4308216%2C4308226%2C4309597%2C4313006%2C4315873%2C4317816%2C4318264%2C4319577%2C4320537%2C4324289%2C4270859%2C4284970%2C4291517%2C4292955%2C4307997&hl=en&gl=us&un=1&otf=1&dest_mid=%2Fm%2F03gh4&dest_state_type=sattd#ttdm=20.321388_-157.381472_7&ttdmf=%252525252Fm%252525252F02lyz9'\n",
    "\n",
    "    soup = get_trip_soup(google_url)  # get soup data\n",
    "    sights_all_data = google_data(soup)  # launch the Google data scraper\n",
    "\n",
    "    for i in sights_all_data:\n",
    "        out = get_geometry(i)\n",
    "\n",
    "        lat = out['candidates'][0]['geometry']['location']['lat']\n",
    "        lng = out['candidates'][0]['geometry']['location']['lng']\n",
    "        sights_all_data[i].append(lat)\n",
    "        sights_all_data[i].append(lng)\n",
    "\n",
    "    # write the dict to a csv file so that source -local can access\n",
    "    with open(\"Sights.csv\", \"w\", newline = '', encoding = 'utf-8') as f1:\n",
    "        csv_output = csv.writer(f1)\n",
    "        csv_output.writerow(['sight_name', 'sight_rating', 'sight_reviews', 'LAT', 'LNG'])  # attributes write to the csv file\n",
    "        for key in sights_all_data.keys():\n",
    "            csv_output.writerow([key] + sights_all_data[key])\n",
    "\n",
    "    # data modeled as dataframe\n",
    "    sight_df = pd.read_csv('Sights.csv')        \n",
    "    return sight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access Google Place API to obtain the geometry data of the hotels and write all the attributes to a csv file\n",
    "def request_hotel_data():\n",
    "    tripadvisor_url = 'https://www.tripadvisor.com/Hotels-g28932-Hawaii-Hotels.html#LEAF_GEO_LIST'\n",
    "    \n",
    "    trip_soup = get_trip_soup(tripadvisor_url)  # get soup data\n",
    "    data = trip_data(trip_soup)  # launch the TripAdvisor data scraper\n",
    "\n",
    "    for i in data:\n",
    "\n",
    "        try:\n",
    "            out = get_geometry(i)\n",
    "            lat = out['candidates'][0]['geometry']['location']['lat']\n",
    "            lng = out['candidates'][0]['geometry']['location']['lng']\n",
    "\n",
    "        # if Google cannot find geometry data for a specific hotel name, that hotel data will be excluded\n",
    "        except:  \n",
    "            print('Google cannot find the location info for', i)\n",
    "            data[i].append(None)\n",
    "            data[i].append(None)\n",
    "\n",
    "        else:\n",
    "            data[i].append(lat)\n",
    "            data[i].append(lng)\n",
    "\n",
    "    # write the dict to a csv file so that source -local can access\n",
    "    with open(\"Hotels.csv\", \"w\", newline = '', encoding = 'utf-8') as f2:\n",
    "        csv_output = csv.writer(f2)\n",
    "        csv_output.writerow(['hotel_name', 'hotel_url', 'hotel_price', 'hotel_rating', 'hotel_review', 'LAT', 'LNG'])  # attributes write to the csv file\n",
    "        for key in data.keys():\n",
    "            csv_output.writerow([key] + data[key])\n",
    "\n",
    "    # data modeled as dataframe\n",
    "    hotel_df = pd.read_csv('Hotels.csv')     \n",
    "    return hotel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-source {local,remote}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/linle66/Library/Jupyter/runtime/kernel-32553b36-6d18-4c17-9926-9919dc194623.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linle66/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-source\", choices = [\"local\", \"remote\"], help = \"where data should be gotten from\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    location = args.source\n",
    "\n",
    "    if location == \"local\":\n",
    "        # local mode\n",
    "        \n",
    "        # since local doesn't get data from webpages or API, it obtains data from local\n",
    "        # load local files and model data as dataframe\n",
    "        sight_df = pd.read_csv('Sights.csv')\n",
    "        hotel_df = pd.read_csv('Hotels.csv')\n",
    "        \n",
    "    else:\n",
    "        # remote mode\n",
    "        \n",
    "        # this code will scrape data from Google and TripAdvisor, then access Google Place API to obtain \n",
    "        # geolocation of places, and eventually create two dataframes to hold the sight data and the hotel data. \n",
    "        # It will also write the data to two csv data files, so that the local mode can access.\n",
    "        sight_df = request_sight_data()\n",
    "        hotel_df = request_hotel_data()\n",
    "        \n",
    "    # Milestone 3\n",
    "        \n",
    "    # get the z score of the sight review variable\n",
    "    sight_df['z_sight_review'] = (sight_df['sight_reviews'] - sight_df['sight_reviews'].mean())/sight_df['sight_reviews'].std()\n",
    "        \n",
    "    # generate the recommendation metric, taking the product of sight rating and the z score of sight review\n",
    "    sight_df['rec_sights'] = sight_df['z_sight_review']*sight_df['sight_rating']\n",
    "        \n",
    "    # get the z score of the hotel review variable\n",
    "    hotel_df['z_hotel_review'] = (hotel_df['hotel_review'] - hotel_df['hotel_review'].mean())/hotel_df['hotel_review'].std()\n",
    "        \n",
    "    # generate the recommendation metric, taking the product of hotel rating and the z score of hotel review\n",
    "    hotel_df['rec_hotels'] = hotel_df['z_hotel_review']*hotel_df['hotel_rating']\n",
    "\n",
    "    # create a html file to store the three interactive maps\n",
    "    output_file(\"Hawaii Sights and Hotels.html\")\n",
    "\n",
    "    # set the map at Hawaii (with the geometry data provided), using Google terrain map as tile with a zoom level of 7\n",
    "    map_options = GMapOptions(lat = 20.716179, lng = -158.214676, map_type = \"terrain\", zoom = 7)\n",
    "\n",
    "    # this package takes in ColumnDataSource format, here converts our dataframe to the corresponding format to provide the data sources for plotting\n",
    "    # sort the sight data by the recommendation metric, and select the 20 most recommended sights\n",
    "    source1 = ColumnDataSource(sight_df.sort_values(by = ['rec_sights'], ascending = False)[:20])\n",
    "    source2 = ColumnDataSource(hotel_df)\n",
    "\n",
    "    # plot Recommended Sights with Hawaii Hotels by Price\n",
    "        \n",
    "    # generate a Hawaii base map by making a request to Google MAP API, and specify the title for the map\n",
    "    p1 = gmap(API_key, map_options, title = \"Recommended Sights with Hawaii Hotels by Price\")\n",
    "\n",
    "    # create triangle objects to represent the sights, specifying the geometry data as x- and y-axis,\n",
    "    # set the color, size of the triangles, also provide a legend to the map\n",
    "    o1 = p1.triangle(x = \"LNG\", y = \"LAT\", size = 9, legend = 'sight', fill_color = \"blue\", fill_alpha = 0.8, source = source1)\n",
    "    # add the hovertool so that when the mouse cursor is at a sight, the sight name will be displayed\n",
    "    p1.add_tools(HoverTool(renderers = [o1], tooltips = [('Sight', '@sight_name')]))\n",
    "\n",
    "    # specify the color for the hotel circles\n",
    "    color_mapper_price = LinearColorMapper(palette = 'BuPu9', low = hotel_df['hotel_price'].min(), high = hotel_df['hotel_price'].max())\n",
    "        \n",
    "    # create a color bar to indicate the relationship between color and price\n",
    "    color_bar_price = ColorBar(color_mapper = color_mapper_price, label_standoff = 12, location = (0,0), title = 'Price')\n",
    "        \n",
    "    # create circle objects to represent the hotels, specifying the geometry data as x- and y-axis,\n",
    "    # set the color, size of the circles, also provide a legend to the map\n",
    "    o2 = p1.circle(x = \"LNG\", y = \"LAT\", size = 7, legend = 'hotel', color = transform('hotel_price', color_mapper_price), alpha = 0.8, source = source2)\n",
    "    # add the hovertool so that when the mouse cursor is at a hotel, the hotel name and lowest price will be displayed\n",
    "    p1.add_tools(HoverTool(renderers = [o2], tooltips = [('Hotel', '@hotel_name'),\n",
    "                                                            ('Price', '@hotel_price')]))\n",
    "\n",
    "    # add a taptool that enable user to click a hotel circle and jump to the hotel webpage in TripAdvisor\n",
    "    url = '@hotel_url'  # specify the url source in our ColumnDataSource\n",
    "    tap = TapTool(callback = OpenURL(url = url))  # specify the open link action for this taptool\n",
    "        \n",
    "    # avoid the hotel circles being grey out during any interations\n",
    "    o2.selection_glyph = None\n",
    "    o2.nonselection_glyph = None\n",
    "        \n",
    "    # add the taptool to the plot\n",
    "    p1.add_tools(tap)\n",
    "\n",
    "    # add the color bar next to the plot\n",
    "    p1.add_layout(color_bar_price, 'right')\n",
    "\n",
    "    # similar to the above, plot Recommended Sights with Hawaii Hotels by Popularity Metric       \n",
    "    p2 = gmap(API_key, map_options, title = \"Recommended Sights with Hawaii Hotels by Popularity Metric\")\n",
    "    o3 = p2.triangle(x = \"LNG\", y = \"LAT\", size = 9, legend = 'sight', fill_color = \"blue\", fill_alpha = 0.8, source = source1)\n",
    "    p2.add_tools(HoverTool(renderers = [o3], tooltips = [('Sight', '@sight_name')]))\n",
    "    color_mapper_pop = LinearColorMapper(palette = 'Viridis9', low = hotel_df['hotel_review'].min(), high = hotel_df['hotel_review'].max())\n",
    "    color_bar_pop = ColorBar(color_mapper = color_mapper_pop, label_standoff = 12, location = (0,0), title = 'Popularity')\n",
    "    o4 = p2.circle(x = \"LNG\", y = \"LAT\", size = 7, legend = 'hotel', color = transform('hotel_review', color_mapper_pop), alpha = 0.8, source = source2)\n",
    "    p2.add_tools(HoverTool(renderers = [o4], tooltips = [('Hotel', '@hotel_name'),\n",
    "                                                            ('Price', '@hotel_price')]))\n",
    "    url = '@hotel_url'\n",
    "    tap = TapTool(callback = OpenURL(url = url))\n",
    "    o4.selection_glyph = None\n",
    "    o4.nonselection_glyph = None\n",
    "    p2.add_tools(tap)\n",
    "    p2.add_layout(color_bar_pop, 'right')\n",
    "        \n",
    "    # similar to the above, plot Recommended Sights with Hawaii Hotels by Recommendation Metric\n",
    "    p3 = gmap(API_key, map_options, title = \"Recommended Sights with Hawaii Hotels by Recommendation Metric\")\n",
    "    o5 = p3.triangle(x = \"LNG\", y = \"LAT\", size = 9, legend = 'sight', fill_color = \"blue\", fill_alpha = 0.8, source = source1)\n",
    "    p3.add_tools(HoverTool(renderers = [o5], tooltips = [('Sight', '@sight_name')]))\n",
    "    color_mapper_pop = LinearColorMapper(palette = 'PiYG9', low = hotel_df['rec_hotels'].min(), high = hotel_df['rec_hotels'].max())\n",
    "    color_bar_pop = ColorBar(color_mapper = color_mapper_pop, label_standoff = 12, location = (0,0), title = 'Recommendation')\n",
    "    o6 = p3.circle(x = \"LNG\", y = \"LAT\", size = 7, legend = 'hotel', color = transform('rec_hotels', color_mapper_pop), alpha = 0.8, source = source2)\n",
    "    p3.add_tools(HoverTool(renderers = [o6], tooltips = [('Hotel', '@hotel_name'),\n",
    "                                                            ('Price', '@hotel_price')]))\n",
    "    url = '@hotel_url'\n",
    "    tap = TapTool(callback = OpenURL(url = url))\n",
    "    o6.selection_glyph = None\n",
    "    o6.nonselection_glyph = None\n",
    "    p3.add_tools(tap)\n",
    "    p3.add_layout(color_bar_pop, 'right')\n",
    "\n",
    "    # automatically pop up the interactive plots in the browser, displayed in a row\n",
    "    show(row(p1,p2,p3))\n",
    "        \n",
    "    # store all data to two csv files\n",
    "    sight_df.to_csv(\"Google_sights_data_complete.csv\", index=False)\n",
    "    hotel_df.to_csv(\"Hotel_data_complete.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
